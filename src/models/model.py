# Data manipulation
import numpy as np
import pandas as pd

# Graphs and networks
import networkx as nx
import community as community_louvain

# Machine Learning
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn import preprocessing
from sklearn.manifold import TSNE
from sklearn.cluster import DBSCAN
from sklearn.neighbors import NearestNeighbors
from sklearn.ensemble import RandomForestClassifier
from sklearn.inspection import permutation_importance
from sklearn.calibration import CalibratedClassifierCV
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV

# Model evaluation
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix

# Data splitting
from sklearn.model_selection import train_test_split

# Visualization
import seaborn as sns
import matplotlib.pyplot as plt


df = pd.read_csv("../data/final.csv")

# for i in df.columns:
#     print("information of: \n", i)
#     print("-" * 50)
#     print("Data \n", df[i].head())
#     print("-" * 50)
#     print("valores_unicos: \n", df[i].unique())
#     print("_" * 50)


# now everythin is perfect
df_dummies = pd.get_dummies(df)


df_dummies.isna().sum().sum()

df1 = df_dummies.copy()
df1 = df1.drop("apoyo", axis=1)


# fit t-SNE on the original (non-scaled) data
tsne = TSNE(n_components=2, verbose=1, n_iter=300, perplexity=30)
tsne_results = tsne.fit_transform(df1)

# add t-SNE results to your dataframe
datos = df_dummies.copy()
datos["tsne-2d-one"] = tsne_results[:, 0]
datos["tsne-2d-two"] = tsne_results[:, 1]

# fit DBSCAN on the t-SNE results
# Increase eps for larger clusters, decrease min_samples for more clusters
c = DBSCAN(eps=2, min_samples=2)
c.fit(tsne_results)
clusters = c.labels_

datos["clusters"] = pd.DataFrame(clusters)[0]

# the remaining code stays the same
clusters_relevantes = datos["clusters"].isin(
    datos["clusters"].value_counts()[datos["clusters"].value_counts() > 1].index
)
datos["clusters_depurado"] = datos["clusters"]
datos.loc[~clusters_relevantes, "clusters_depurado"] = -1


fig, axs = plt.subplots(figsize=(16, 5))

sns.scatterplot(
    x="tsne-2d-one",
    y="tsne-2d-two",
    hue="clusters_depurado",
    palette=sns.color_palette("hls", len(datos["clusters_depurado"].unique())),
    data=datos,
    legend="full",
    alpha=1,
)


# Predict the clusters

X = df_dummies
y = datos["clusters_depurado"]

X_nonoise = X[y != -1].copy()
y_nonoise = y[y != -1].copy()


X1_train, X1_test, y1_train, y1_test = train_test_split(
    X_nonoise, y_nonoise, test_size=0.2
)

xgb = XGBClassifier(
    learning_rate=0.01,
    colsample_bytree=0.4,
    subsample=0.8,
    objective="multi:softmax",
    n_estimators=1000,
    reg_alpha=0.3,
    max_depth=4,
    gamma=10,
)

y1_pred = xgb.fit(X1_train, y1_train).predict(X1_test)

print("Accuracy: ", accuracy_score(y1_test, y1_pred))

xgb.fit(X1_train, y1_train)

datos["cluster_predict"] = xgb.predict(X)
fig, axs = plt.subplots(figsize=(16, 5))

sns.scatterplot(
    x="tsne-2d-one",
    y="tsne-2d-two",
    hue="cluster_predict",
    palette=sns.color_palette("hls", len(datos["cluster_predict"].unique())),
    data=datos,
    legend="full",
    alpha=1,
)


# What cluster is belong to the apoyo == 1

# Create a mask where 'apoyo' is 1
mask_apoyo = datos["apoyo"] == 1

# Apply the mask to your DataFrame to filter the rows where 'apoyo' is 1
datos_apoyo = datos[mask_apoyo]

# Count the number of 'apoyo == 1' in each cluster
cluster_distribution = datos_apoyo["clusters_depurado"].value_counts()

# This will give you the cluster with the most 'apoyo == 1'
max_apoyo_cluster = cluster_distribution.idxmax()

# Print the cluster with the most 'apoyo == 1'
print(f'The cluster with the most "apoyo == 1" is: {max_apoyo_cluster}')


# Now with the apoyo column

y = df["apoyo"]

# Drop 'apoyo' from your df_dummies dataframe to use the rest of the columns as your features.
X = df_dummies.drop("apoyo", axis=1)

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

xgb = XGBClassifier(
    learning_rate=0.01,
    colsample_bytree=0.4,
    subsample=0.8,
    objective="binary:logistic",
    n_estimators=1000,
    reg_alpha=0.3,
    max_depth=4,
    gamma=10,
)


# Fit and predict
xgb.fit(X_train, y_train)
y_pred = xgb.predict(X_test)


# Get the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("Truth")
plt.show()


# Compute ROC curve and ROC area
fpr, tpr, _ = roc_curve(y_test, xgb.predict_proba(X_test)[:, 1])
roc_auc = auc(fpr, tpr)

# Plot
plt.figure()
plt.plot(fpr, tpr, color="darkorange", label="ROC curve (area = %0.2f)" % roc_auc)
plt.plot([0, 1], [0, 1], color="navy", linestyle="--")
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Receiver Operating Characteristic")
plt.legend(loc="lower right")
plt.show()


# importante
# Get feature importances
importances = xgb.feature_importances_

# Convert the importances into one-dimensional 1darray with corresponding df column names as axis labels
f_importances = pd.Series(importances, X.columns)

# Sort the array in descending order of the importances
f_importances.sort_values(ascending=True, inplace=True)

# Make the bar Plot from f_importances
f_importances.plot(kind="barh", figsize=(9, 16))

# Show the plot
plt.xlabel("Importance")
plt.title("Feature Importances")
plt.tight_layout()
plt.show()


# Compute ROC curve and ROC area for several models

# Define the classifiers
classifiers = {
    "XGBoost": XGBClassifier(
        learning_rate=0.01,
        colsample_bytree=0.4,
        subsample=0.8,
        objective="binary:logistic",
        n_estimators=1000,
        reg_alpha=0.3,
        max_depth=4,
        gamma=10,
    ),
    "Random Forest": RandomForestClassifier(
        n_estimators=1000,
        max_depth=4,
    ),
    "SVC": CalibratedClassifierCV(SVC(probability=True)),
}

roc_data = []

# Fit, predict and compute ROC curve for each classifier
for name, clf in classifiers.items():
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)

    # Compute ROC curve and ROC area
    fpr, tpr, _ = roc_curve(y_test, clf.predict_proba(X_test)[:, 1])
    roc_auc = auc(fpr, tpr)

    # Save ROC data
    roc_data.append((fpr, tpr, roc_auc, name))

    # Get the confusion matrix
    cm = confusion_matrix(y_test, y_pred)

    # Plot the confusion matrix
    plt.figure()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel("Predicted")
    plt.ylabel("Truth")
    plt.title(f"{name} Confusion Matrix")
    plt.show()


# Initialize the plot for the ROC curve
plt.figure(figsize=(10, 7))

# Plot ROC curve for each classifier
for fpr, tpr, roc_auc, name in roc_data:
    plt.plot(fpr, tpr, label="%s ROC curve (area = %0.2f)" % (name, roc_auc))

# Finalize the plot for the ROC curve
plt.plot([0, 1], [0, 1], color="navy", linestyle="--")
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Receiver Operating Characteristic")
plt.legend(loc="lower right")
plt.show()
